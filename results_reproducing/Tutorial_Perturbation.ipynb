{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Pre-trained Model for Perturbation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gears import PertData, GEARS\n",
    "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerGenerator\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    criterion_neg_log_bernoulli,\n",
    "    masked_relative_error,\n",
    ")\n",
    "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.utils import set_seed, map_raw_id_to_vocab_id\n",
    "\n",
    "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# settings for data prcocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 2\n",
    "\n",
    "n_hvg = 0  # number of highly variable genes\n",
    "include_zero_gene = \"all\"  # include zero expr genes in training input, \"all\", \"batch-wise\", \"row-wise\", or False\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "cell_emb_style = \"cls\"\n",
    "mvc_decoder_style = \"inner product, detach\"\n",
    "amp = True\n",
    "load_model = \"save/scGPT_human2\"\n",
    "load_param_prefixs = [\n",
    "    \"encoder\",\n",
    "    \"value_encoder\",\n",
    "    \"transformer_encoder\",\n",
    "]\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4  # or 1e-4\n",
    "batch_size = 10\n",
    "# batch_size = 64\n",
    "eval_batch_size = 3\n",
    "# eval_batch_size = 64\n",
    "epochs = 15\n",
    "schedule_interval = 1\n",
    "early_stop = 5\n",
    "\n",
    "# settings for the model\n",
    "embsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "n_layers_cls = 3\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!1\n",
    "# use_fast_transformer = True  # whether to use fast transformer\n",
    "use_fast_transformer = False  # whether to use fast transformer\n",
    "\n",
    "# logging\n",
    "log_interval = 100\n",
    "\n",
    "# dataset and evaluation choices\n",
    "data_name = \"adamson\"\n",
    "split = \"simulation\"\n",
    "if data_name == \"norman\":\n",
    "    perts_to_plot = [\"SAMD1+ZBTB1\"]\n",
    "elif data_name == \"adamson\":\n",
    "    perts_to_plot = [\"KCTD16+ctrl\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to save/dev_perturb_adamson-May12-13-22\n",
      "scGPT - INFO - Running on 2024-05-12 13:22:14\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(f\"./save/dev_perturb_{data_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"saving to {save_dir}\")\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "# log running date and current git commit\n",
    "logger.info(f\"Running on {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:22\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data = PertData(\"./data\")\n",
    "pert_data.load(data_name=data_name)\n",
    "pert_data.prepare_split(split=split, seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 4399/5060 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from save/scGPT_human2/best_model.pt, the model args will override the config save/scGPT_human2/args.json.\n"
     ]
    }
   ],
   "source": [
    "if load_model is not None:\n",
    "    model_dir = Path(load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    pert_data.adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in pert_data.adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(pert_data.adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "else:\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    ")\n",
    "n_genes = len(genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Create and train scGpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using simple batchnorm instead of domain specific batchnorm\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerGenerator(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pert_encoder): Embedding(3, 512, padding_idx=2)\n",
       "  (bn): BatchNorm1d(512, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (6): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (7): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (8): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (9): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (10): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (11): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    do_mvc=MVC,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "\n",
    "load_param_prefixs = None\n",
    "\n",
    "if load_param_prefixs is not None and load_model is not None:\n",
    "    # only load params that start with the prefix\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file)\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if any([k.startswith(prefix) for prefix in load_param_prefixs])\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "elif load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# summary(model, (3, 60697))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, schedule_interval, gamma=0.9)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "\n",
    "def train(model: nn.Module, train_loader: torch.utils.data.DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse = 0.0, 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    for batch, batch_data in enumerate(train_loader):\n",
    "        batch_size = len(batch_data.y)\n",
    "        batch_data.to(device)\n",
    "        x: torch.Tensor = batch_data.x  # (batch_size * n_genes, 2)\n",
    "        ori_gene_values = x[:, 0].view(batch_size, n_genes)\n",
    "        pert_flags = x[:, 1].long().view(batch_size, n_genes)\n",
    "        target_gene_values = batch_data.y  # (batch_size, n_genes)\n",
    "\n",
    "        if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
    "            if include_zero_gene == \"all\":\n",
    "                input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n",
    "            else:\n",
    "                input_gene_ids = (\n",
    "                    ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
    "                )\n",
    "            # sample input_gene_id\n",
    "            if len(input_gene_ids) > max_seq_len:\n",
    "                input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
    "                    :max_seq_len\n",
    "                ]\n",
    "            input_values = ori_gene_values[:, input_gene_ids]\n",
    "            input_pert_flags = pert_flags[:, input_gene_ids]\n",
    "            target_values = target_gene_values[:, input_gene_ids]\n",
    "\n",
    "            mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
    "            mapped_input_gene_ids = mapped_input_gene_ids.repeat(batch_size, 1)\n",
    "\n",
    "            # src_key_padding_mask = mapped_input_gene_ids.eq(vocab[pad_token])\n",
    "            src_key_padding_mask = torch.zeros_like(\n",
    "                input_values, dtype=torch.bool, device=device\n",
    "            )\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output_dict = model(\n",
    "                mapped_input_gene_ids,\n",
    "                input_values,\n",
    "                input_pert_flags,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "            )\n",
    "            output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "            masked_positions = torch.ones_like(\n",
    "                input_values, dtype=torch.bool\n",
    "            )  # Use all\n",
    "            loss = loss_mse = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} |\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, val_loader: torch.utils.data.DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_data in enumerate(val_loader):\n",
    "            batch_size = len(batch_data.y)\n",
    "            batch_data.to(device)\n",
    "            x: torch.Tensor = batch_data.x  # (batch_size * n_genes, 2)\n",
    "            ori_gene_values = x[:, 0].view(batch_size, n_genes)\n",
    "            pert_flags = x[:, 1].long().view(batch_size, n_genes)\n",
    "            target_gene_values = batch_data.y  # (batch_size, n_genes)\n",
    "\n",
    "            if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
    "                if include_zero_gene == \"all\":\n",
    "                    input_gene_ids = torch.arange(n_genes, device=device)\n",
    "                else:  # when batch-wise\n",
    "                    input_gene_ids = (\n",
    "                        ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
    "                    )\n",
    "\n",
    "                # sample input_gene_id\n",
    "                if len(input_gene_ids) > max_seq_len:\n",
    "                    input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
    "                        :max_seq_len\n",
    "                    ]\n",
    "                input_values = ori_gene_values[:, input_gene_ids]\n",
    "                input_pert_flags = pert_flags[:, input_gene_ids]\n",
    "                target_values = target_gene_values[:, input_gene_ids]\n",
    "\n",
    "                mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
    "                mapped_input_gene_ids = mapped_input_gene_ids.repeat(batch_size, 1)\n",
    "\n",
    "                # src_key_padding_mask = mapped_input_gene_ids.eq(vocab[pad_token])\n",
    "                src_key_padding_mask = torch.zeros_like(\n",
    "                    input_values, dtype=torch.bool, device=input_values.device\n",
    "                )\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output_dict = model(\n",
    "                    mapped_input_gene_ids,\n",
    "                    input_values,\n",
    "                    input_pert_flags,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    CLS=CLS,\n",
    "                    CCE=CCE,\n",
    "                    MVC=MVC,\n",
    "                    ECS=ECS,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = torch.ones_like(\n",
    "                    input_values, dtype=torch.bool, device=input_values.device\n",
    "                )\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "            total_loss += loss.item()\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item()\n",
    "    return total_loss / len(val_loader), total_error / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch   1 | 100/5435 batches | lr 0.0001 | ms/batch 358.98 | loss 95.69 | mse 95.69 |\n",
      "scGPT - INFO - | epoch   1 | 200/5435 batches | lr 0.0001 | ms/batch 350.62 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   1 | 300/5435 batches | lr 0.0001 | ms/batch 350.70 | loss  0.10 | mse  0.10 |\n",
      "scGPT - INFO - | epoch   1 | 400/5435 batches | lr 0.0001 | ms/batch 350.71 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 500/5435 batches | lr 0.0001 | ms/batch 351.07 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 600/5435 batches | lr 0.0001 | ms/batch 351.41 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 700/5435 batches | lr 0.0001 | ms/batch 351.28 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 800/5435 batches | lr 0.0001 | ms/batch 351.51 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 900/5435 batches | lr 0.0001 | ms/batch 351.57 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 1000/5435 batches | lr 0.0001 | ms/batch 351.55 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 1100/5435 batches | lr 0.0001 | ms/batch 351.97 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 1200/5435 batches | lr 0.0001 | ms/batch 351.10 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 1300/5435 batches | lr 0.0001 | ms/batch 350.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 1400/5435 batches | lr 0.0001 | ms/batch 350.36 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 1500/5435 batches | lr 0.0001 | ms/batch 350.07 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 1600/5435 batches | lr 0.0001 | ms/batch 350.43 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 1700/5435 batches | lr 0.0001 | ms/batch 350.44 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 1800/5435 batches | lr 0.0001 | ms/batch 350.19 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 1900/5435 batches | lr 0.0001 | ms/batch 350.32 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 2000/5435 batches | lr 0.0001 | ms/batch 350.57 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 2100/5435 batches | lr 0.0001 | ms/batch 350.46 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 2200/5435 batches | lr 0.0001 | ms/batch 349.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 2300/5435 batches | lr 0.0001 | ms/batch 350.63 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 2400/5435 batches | lr 0.0001 | ms/batch 350.34 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 2500/5435 batches | lr 0.0001 | ms/batch 350.24 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 2600/5435 batches | lr 0.0001 | ms/batch 350.62 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 2700/5435 batches | lr 0.0001 | ms/batch 350.09 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 2800/5435 batches | lr 0.0001 | ms/batch 350.58 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   1 | 2900/5435 batches | lr 0.0001 | ms/batch 350.41 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3000/5435 batches | lr 0.0001 | ms/batch 350.29 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3100/5435 batches | lr 0.0001 | ms/batch 350.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3200/5435 batches | lr 0.0001 | ms/batch 350.29 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3300/5435 batches | lr 0.0001 | ms/batch 350.15 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3400/5435 batches | lr 0.0001 | ms/batch 350.24 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3500/5435 batches | lr 0.0001 | ms/batch 350.10 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3600/5435 batches | lr 0.0001 | ms/batch 350.07 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3700/5435 batches | lr 0.0001 | ms/batch 350.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3800/5435 batches | lr 0.0001 | ms/batch 350.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 3900/5435 batches | lr 0.0001 | ms/batch 349.85 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4000/5435 batches | lr 0.0001 | ms/batch 350.37 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4100/5435 batches | lr 0.0001 | ms/batch 350.10 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4200/5435 batches | lr 0.0001 | ms/batch 350.19 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4300/5435 batches | lr 0.0001 | ms/batch 350.51 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4400/5435 batches | lr 0.0001 | ms/batch 350.57 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4500/5435 batches | lr 0.0001 | ms/batch 350.74 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4600/5435 batches | lr 0.0001 | ms/batch 350.64 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4700/5435 batches | lr 0.0001 | ms/batch 350.63 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4800/5435 batches | lr 0.0001 | ms/batch 350.86 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 4900/5435 batches | lr 0.0001 | ms/batch 350.87 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 5000/5435 batches | lr 0.0001 | ms/batch 350.55 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 5100/5435 batches | lr 0.0001 | ms/batch 350.62 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 5200/5435 batches | lr 0.0001 | ms/batch 350.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 5300/5435 batches | lr 0.0001 | ms/batch 350.57 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   1 | 5400/5435 batches | lr 0.0001 | ms/batch 350.83 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 1937.25s | valid loss/mse 0.1436 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.1436\n",
      "scGPT - INFO - | epoch   2 | 100/5435 batches | lr 0.0001 | ms/batch 353.70 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 200/5435 batches | lr 0.0001 | ms/batch 351.02 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 300/5435 batches | lr 0.0001 | ms/batch 350.35 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 400/5435 batches | lr 0.0001 | ms/batch 350.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 500/5435 batches | lr 0.0001 | ms/batch 350.43 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 600/5435 batches | lr 0.0001 | ms/batch 350.56 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 700/5435 batches | lr 0.0001 | ms/batch 350.84 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 800/5435 batches | lr 0.0001 | ms/batch 350.68 | loss  0.09 | mse  0.09 |\n",
      "scGPT - INFO - | epoch   2 | 900/5435 batches | lr 0.0001 | ms/batch 350.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1000/5435 batches | lr 0.0001 | ms/batch 350.50 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1100/5435 batches | lr 0.0001 | ms/batch 350.38 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1200/5435 batches | lr 0.0001 | ms/batch 351.32 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1300/5435 batches | lr 0.0001 | ms/batch 350.95 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1400/5435 batches | lr 0.0001 | ms/batch 350.81 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1500/5435 batches | lr 0.0001 | ms/batch 351.30 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1600/5435 batches | lr 0.0001 | ms/batch 351.75 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1700/5435 batches | lr 0.0001 | ms/batch 351.72 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1800/5435 batches | lr 0.0001 | ms/batch 351.02 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 1900/5435 batches | lr 0.0001 | ms/batch 350.93 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2000/5435 batches | lr 0.0001 | ms/batch 351.27 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2100/5435 batches | lr 0.0001 | ms/batch 350.64 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2200/5435 batches | lr 0.0001 | ms/batch 351.25 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2300/5435 batches | lr 0.0001 | ms/batch 351.32 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2400/5435 batches | lr 0.0001 | ms/batch 350.97 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2500/5435 batches | lr 0.0001 | ms/batch 350.66 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2600/5435 batches | lr 0.0001 | ms/batch 351.12 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2700/5435 batches | lr 0.0001 | ms/batch 350.55 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2800/5435 batches | lr 0.0001 | ms/batch 350.70 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 2900/5435 batches | lr 0.0001 | ms/batch 351.11 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3000/5435 batches | lr 0.0001 | ms/batch 350.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3100/5435 batches | lr 0.0001 | ms/batch 351.21 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3200/5435 batches | lr 0.0001 | ms/batch 351.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3300/5435 batches | lr 0.0001 | ms/batch 351.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3400/5435 batches | lr 0.0001 | ms/batch 351.08 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3500/5435 batches | lr 0.0001 | ms/batch 351.20 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3600/5435 batches | lr 0.0001 | ms/batch 351.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3700/5435 batches | lr 0.0001 | ms/batch 351.08 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3800/5435 batches | lr 0.0001 | ms/batch 350.93 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 3900/5435 batches | lr 0.0001 | ms/batch 351.09 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4000/5435 batches | lr 0.0001 | ms/batch 350.89 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4100/5435 batches | lr 0.0001 | ms/batch 351.62 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4200/5435 batches | lr 0.0001 | ms/batch 351.32 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4300/5435 batches | lr 0.0001 | ms/batch 351.28 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4400/5435 batches | lr 0.0001 | ms/batch 351.26 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4500/5435 batches | lr 0.0001 | ms/batch 351.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4600/5435 batches | lr 0.0001 | ms/batch 350.84 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4700/5435 batches | lr 0.0001 | ms/batch 350.52 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4800/5435 batches | lr 0.0001 | ms/batch 350.41 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 4900/5435 batches | lr 0.0001 | ms/batch 350.59 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 5000/5435 batches | lr 0.0001 | ms/batch 351.10 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 5100/5435 batches | lr 0.0001 | ms/batch 350.82 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 5200/5435 batches | lr 0.0001 | ms/batch 350.71 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 5300/5435 batches | lr 0.0001 | ms/batch 351.08 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   2 | 5400/5435 batches | lr 0.0001 | ms/batch 351.02 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 1938.91s | valid loss/mse 0.1320 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.1320\n",
      "scGPT - INFO - | epoch   3 | 100/5435 batches | lr 0.0001 | ms/batch 354.42 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 200/5435 batches | lr 0.0001 | ms/batch 350.65 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 300/5435 batches | lr 0.0001 | ms/batch 351.25 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 400/5435 batches | lr 0.0001 | ms/batch 350.62 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 500/5435 batches | lr 0.0001 | ms/batch 351.13 | loss  0.07 | mse  0.07 |\n",
      "scGPT - INFO - | epoch   3 | 600/5435 batches | lr 0.0001 | ms/batch 350.79 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 700/5435 batches | lr 0.0001 | ms/batch 350.61 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 800/5435 batches | lr 0.0001 | ms/batch 350.63 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 900/5435 batches | lr 0.0001 | ms/batch 350.87 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1000/5435 batches | lr 0.0001 | ms/batch 351.15 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1100/5435 batches | lr 0.0001 | ms/batch 350.92 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1200/5435 batches | lr 0.0001 | ms/batch 351.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1300/5435 batches | lr 0.0001 | ms/batch 351.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1400/5435 batches | lr 0.0001 | ms/batch 350.89 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1500/5435 batches | lr 0.0001 | ms/batch 350.78 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1600/5435 batches | lr 0.0001 | ms/batch 350.73 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1700/5435 batches | lr 0.0001 | ms/batch 351.38 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1800/5435 batches | lr 0.0001 | ms/batch 350.89 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 1900/5435 batches | lr 0.0001 | ms/batch 350.93 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2000/5435 batches | lr 0.0001 | ms/batch 351.26 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2100/5435 batches | lr 0.0001 | ms/batch 351.06 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2200/5435 batches | lr 0.0001 | ms/batch 351.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2300/5435 batches | lr 0.0001 | ms/batch 350.60 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2400/5435 batches | lr 0.0001 | ms/batch 350.70 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2500/5435 batches | lr 0.0001 | ms/batch 350.67 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2600/5435 batches | lr 0.0001 | ms/batch 350.67 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2700/5435 batches | lr 0.0001 | ms/batch 350.98 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2800/5435 batches | lr 0.0001 | ms/batch 350.73 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 2900/5435 batches | lr 0.0001 | ms/batch 351.01 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3000/5435 batches | lr 0.0001 | ms/batch 351.01 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3100/5435 batches | lr 0.0001 | ms/batch 350.83 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3200/5435 batches | lr 0.0001 | ms/batch 351.04 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3300/5435 batches | lr 0.0001 | ms/batch 350.94 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3400/5435 batches | lr 0.0001 | ms/batch 351.24 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3500/5435 batches | lr 0.0001 | ms/batch 351.15 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3600/5435 batches | lr 0.0001 | ms/batch 351.02 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3700/5435 batches | lr 0.0001 | ms/batch 351.38 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3800/5435 batches | lr 0.0001 | ms/batch 351.00 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 3900/5435 batches | lr 0.0001 | ms/batch 351.29 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4000/5435 batches | lr 0.0001 | ms/batch 351.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4100/5435 batches | lr 0.0001 | ms/batch 351.05 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4200/5435 batches | lr 0.0001 | ms/batch 351.13 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4300/5435 batches | lr 0.0001 | ms/batch 351.05 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4400/5435 batches | lr 0.0001 | ms/batch 351.28 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4500/5435 batches | lr 0.0001 | ms/batch 351.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4600/5435 batches | lr 0.0001 | ms/batch 351.21 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4700/5435 batches | lr 0.0001 | ms/batch 350.72 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4800/5435 batches | lr 0.0001 | ms/batch 350.96 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 4900/5435 batches | lr 0.0001 | ms/batch 350.78 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 5000/5435 batches | lr 0.0001 | ms/batch 350.61 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 5100/5435 batches | lr 0.0001 | ms/batch 350.96 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 5200/5435 batches | lr 0.0001 | ms/batch 350.96 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 5300/5435 batches | lr 0.0001 | ms/batch 351.11 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   3 | 5400/5435 batches | lr 0.0001 | ms/batch 351.17 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 1939.03s | valid loss/mse 0.1356 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   4 | 100/5435 batches | lr 0.0001 | ms/batch 354.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 200/5435 batches | lr 0.0001 | ms/batch 351.18 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 300/5435 batches | lr 0.0001 | ms/batch 351.12 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 400/5435 batches | lr 0.0001 | ms/batch 351.30 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 500/5435 batches | lr 0.0001 | ms/batch 351.09 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 600/5435 batches | lr 0.0001 | ms/batch 351.43 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 700/5435 batches | lr 0.0001 | ms/batch 351.47 | loss  0.07 | mse  0.07 |\n",
      "scGPT - INFO - | epoch   4 | 800/5435 batches | lr 0.0001 | ms/batch 351.55 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 900/5435 batches | lr 0.0001 | ms/batch 351.28 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1000/5435 batches | lr 0.0001 | ms/batch 351.01 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1100/5435 batches | lr 0.0001 | ms/batch 351.06 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1200/5435 batches | lr 0.0001 | ms/batch 350.78 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1300/5435 batches | lr 0.0001 | ms/batch 350.91 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1400/5435 batches | lr 0.0001 | ms/batch 350.80 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1500/5435 batches | lr 0.0001 | ms/batch 351.21 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1600/5435 batches | lr 0.0001 | ms/batch 350.95 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1700/5435 batches | lr 0.0001 | ms/batch 351.02 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1800/5435 batches | lr 0.0001 | ms/batch 351.06 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 1900/5435 batches | lr 0.0001 | ms/batch 350.94 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2000/5435 batches | lr 0.0001 | ms/batch 350.72 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2100/5435 batches | lr 0.0001 | ms/batch 351.25 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2200/5435 batches | lr 0.0001 | ms/batch 351.20 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2300/5435 batches | lr 0.0001 | ms/batch 351.12 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2400/5435 batches | lr 0.0001 | ms/batch 350.93 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2500/5435 batches | lr 0.0001 | ms/batch 350.89 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2600/5435 batches | lr 0.0001 | ms/batch 351.10 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2700/5435 batches | lr 0.0001 | ms/batch 351.14 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2800/5435 batches | lr 0.0001 | ms/batch 350.86 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 2900/5435 batches | lr 0.0001 | ms/batch 350.67 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3000/5435 batches | lr 0.0001 | ms/batch 350.63 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3100/5435 batches | lr 0.0001 | ms/batch 350.65 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3200/5435 batches | lr 0.0001 | ms/batch 350.54 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3300/5435 batches | lr 0.0001 | ms/batch 350.68 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3400/5435 batches | lr 0.0001 | ms/batch 350.82 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3500/5435 batches | lr 0.0001 | ms/batch 350.99 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3600/5435 batches | lr 0.0001 | ms/batch 350.71 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3700/5435 batches | lr 0.0001 | ms/batch 350.79 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3800/5435 batches | lr 0.0001 | ms/batch 350.73 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 3900/5435 batches | lr 0.0001 | ms/batch 350.75 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4000/5435 batches | lr 0.0001 | ms/batch 350.78 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4100/5435 batches | lr 0.0001 | ms/batch 350.68 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4200/5435 batches | lr 0.0001 | ms/batch 350.83 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4300/5435 batches | lr 0.0001 | ms/batch 350.85 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4400/5435 batches | lr 0.0001 | ms/batch 350.74 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4500/5435 batches | lr 0.0001 | ms/batch 350.89 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4600/5435 batches | lr 0.0001 | ms/batch 351.04 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4700/5435 batches | lr 0.0001 | ms/batch 350.65 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4800/5435 batches | lr 0.0001 | ms/batch 350.65 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 4900/5435 batches | lr 0.0001 | ms/batch 350.37 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 5000/5435 batches | lr 0.0001 | ms/batch 350.96 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 5100/5435 batches | lr 0.0001 | ms/batch 350.77 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 5200/5435 batches | lr 0.0001 | ms/batch 350.83 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 5300/5435 batches | lr 0.0001 | ms/batch 350.57 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   4 | 5400/5435 batches | lr 0.0001 | ms/batch 350.94 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 1938.67s | valid loss/mse 0.1336 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   5 | 100/5435 batches | lr 0.0001 | ms/batch 354.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 200/5435 batches | lr 0.0001 | ms/batch 350.84 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 300/5435 batches | lr 0.0001 | ms/batch 350.28 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 400/5435 batches | lr 0.0001 | ms/batch 351.04 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 500/5435 batches | lr 0.0001 | ms/batch 350.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 600/5435 batches | lr 0.0001 | ms/batch 350.25 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 700/5435 batches | lr 0.0001 | ms/batch 350.45 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 800/5435 batches | lr 0.0001 | ms/batch 350.32 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 900/5435 batches | lr 0.0001 | ms/batch 350.40 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1000/5435 batches | lr 0.0001 | ms/batch 350.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1100/5435 batches | lr 0.0001 | ms/batch 350.49 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1200/5435 batches | lr 0.0001 | ms/batch 350.66 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1300/5435 batches | lr 0.0001 | ms/batch 350.55 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1400/5435 batches | lr 0.0001 | ms/batch 350.63 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1500/5435 batches | lr 0.0001 | ms/batch 350.55 | loss  0.07 | mse  0.07 |\n",
      "scGPT - INFO - | epoch   5 | 1600/5435 batches | lr 0.0001 | ms/batch 350.28 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1700/5435 batches | lr 0.0001 | ms/batch 350.25 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1800/5435 batches | lr 0.0001 | ms/batch 350.38 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 1900/5435 batches | lr 0.0001 | ms/batch 350.47 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2000/5435 batches | lr 0.0001 | ms/batch 350.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2100/5435 batches | lr 0.0001 | ms/batch 350.52 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2200/5435 batches | lr 0.0001 | ms/batch 350.21 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2300/5435 batches | lr 0.0001 | ms/batch 350.13 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2400/5435 batches | lr 0.0001 | ms/batch 350.23 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2500/5435 batches | lr 0.0001 | ms/batch 350.55 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2600/5435 batches | lr 0.0001 | ms/batch 350.63 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2700/5435 batches | lr 0.0001 | ms/batch 350.47 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2800/5435 batches | lr 0.0001 | ms/batch 350.53 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 2900/5435 batches | lr 0.0001 | ms/batch 350.62 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3000/5435 batches | lr 0.0001 | ms/batch 350.28 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3100/5435 batches | lr 0.0001 | ms/batch 350.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3200/5435 batches | lr 0.0001 | ms/batch 350.70 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3300/5435 batches | lr 0.0001 | ms/batch 350.48 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3400/5435 batches | lr 0.0001 | ms/batch 350.74 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3500/5435 batches | lr 0.0001 | ms/batch 350.58 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3600/5435 batches | lr 0.0001 | ms/batch 350.48 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3700/5435 batches | lr 0.0001 | ms/batch 350.48 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3800/5435 batches | lr 0.0001 | ms/batch 350.96 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 3900/5435 batches | lr 0.0001 | ms/batch 350.36 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4000/5435 batches | lr 0.0001 | ms/batch 350.63 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4100/5435 batches | lr 0.0001 | ms/batch 350.58 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4200/5435 batches | lr 0.0001 | ms/batch 350.42 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4300/5435 batches | lr 0.0001 | ms/batch 350.80 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4400/5435 batches | lr 0.0001 | ms/batch 350.28 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4500/5435 batches | lr 0.0001 | ms/batch 350.61 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4600/5435 batches | lr 0.0001 | ms/batch 350.58 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4700/5435 batches | lr 0.0001 | ms/batch 350.69 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4800/5435 batches | lr 0.0001 | ms/batch 350.74 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 4900/5435 batches | lr 0.0001 | ms/batch 350.83 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 5000/5435 batches | lr 0.0001 | ms/batch 350.49 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 5100/5435 batches | lr 0.0001 | ms/batch 350.29 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 5200/5435 batches | lr 0.0001 | ms/batch 350.45 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 5300/5435 batches | lr 0.0001 | ms/batch 350.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   5 | 5400/5435 batches | lr 0.0001 | ms/batch 350.45 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 1936.44s | valid loss/mse 0.1369 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   6 | 100/5435 batches | lr 0.0001 | ms/batch 354.01 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 200/5435 batches | lr 0.0001 | ms/batch 350.44 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 300/5435 batches | lr 0.0001 | ms/batch 350.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 400/5435 batches | lr 0.0001 | ms/batch 350.46 | loss  0.07 | mse  0.07 |\n",
      "scGPT - INFO - | epoch   6 | 500/5435 batches | lr 0.0001 | ms/batch 350.57 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 600/5435 batches | lr 0.0001 | ms/batch 350.32 | loss  0.07 | mse  0.07 |\n",
      "scGPT - INFO - | epoch   6 | 700/5435 batches | lr 0.0001 | ms/batch 350.54 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 800/5435 batches | lr 0.0001 | ms/batch 350.50 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 900/5435 batches | lr 0.0001 | ms/batch 350.54 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1000/5435 batches | lr 0.0001 | ms/batch 350.32 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1100/5435 batches | lr 0.0001 | ms/batch 350.23 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1200/5435 batches | lr 0.0001 | ms/batch 350.27 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1300/5435 batches | lr 0.0001 | ms/batch 350.25 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1400/5435 batches | lr 0.0001 | ms/batch 350.29 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1500/5435 batches | lr 0.0001 | ms/batch 350.24 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1600/5435 batches | lr 0.0001 | ms/batch 350.16 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1700/5435 batches | lr 0.0001 | ms/batch 350.62 | loss  0.07 | mse  0.07 |\n",
      "scGPT - INFO - | epoch   6 | 1800/5435 batches | lr 0.0001 | ms/batch 350.32 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 1900/5435 batches | lr 0.0001 | ms/batch 350.34 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2000/5435 batches | lr 0.0001 | ms/batch 350.39 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2100/5435 batches | lr 0.0001 | ms/batch 350.82 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2200/5435 batches | lr 0.0001 | ms/batch 350.47 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2300/5435 batches | lr 0.0001 | ms/batch 350.71 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2400/5435 batches | lr 0.0001 | ms/batch 350.73 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2500/5435 batches | lr 0.0001 | ms/batch 350.56 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2600/5435 batches | lr 0.0001 | ms/batch 350.25 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2700/5435 batches | lr 0.0001 | ms/batch 350.21 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2800/5435 batches | lr 0.0001 | ms/batch 350.31 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 2900/5435 batches | lr 0.0001 | ms/batch 350.80 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3000/5435 batches | lr 0.0001 | ms/batch 350.75 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3100/5435 batches | lr 0.0001 | ms/batch 350.29 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3200/5435 batches | lr 0.0001 | ms/batch 350.67 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3300/5435 batches | lr 0.0001 | ms/batch 350.41 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3400/5435 batches | lr 0.0001 | ms/batch 350.96 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3500/5435 batches | lr 0.0001 | ms/batch 350.43 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3600/5435 batches | lr 0.0001 | ms/batch 350.51 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3700/5435 batches | lr 0.0001 | ms/batch 350.36 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3800/5435 batches | lr 0.0001 | ms/batch 350.54 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 3900/5435 batches | lr 0.0001 | ms/batch 350.33 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4000/5435 batches | lr 0.0001 | ms/batch 350.43 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4100/5435 batches | lr 0.0001 | ms/batch 350.57 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4200/5435 batches | lr 0.0001 | ms/batch 350.72 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4300/5435 batches | lr 0.0001 | ms/batch 350.92 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4400/5435 batches | lr 0.0001 | ms/batch 350.50 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4500/5435 batches | lr 0.0001 | ms/batch 350.54 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4600/5435 batches | lr 0.0001 | ms/batch 350.57 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4700/5435 batches | lr 0.0001 | ms/batch 350.37 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4800/5435 batches | lr 0.0001 | ms/batch 350.55 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 4900/5435 batches | lr 0.0001 | ms/batch 350.35 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 5000/5435 batches | lr 0.0001 | ms/batch 350.20 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 5100/5435 batches | lr 0.0001 | ms/batch 350.49 | loss  0.07 | mse  0.07 |\n",
      "scGPT - INFO - | epoch   6 | 5200/5435 batches | lr 0.0001 | ms/batch 350.21 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 5300/5435 batches | lr 0.0001 | ms/batch 350.30 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   6 | 5400/5435 batches | lr 0.0001 | ms/batch 350.51 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 1936.17s | valid loss/mse 0.1340 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | epoch   7 | 100/5435 batches | lr 0.0001 | ms/batch 354.10 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 200/5435 batches | lr 0.0001 | ms/batch 350.48 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 300/5435 batches | lr 0.0001 | ms/batch 350.61 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 400/5435 batches | lr 0.0001 | ms/batch 350.48 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 500/5435 batches | lr 0.0001 | ms/batch 350.75 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 600/5435 batches | lr 0.0001 | ms/batch 350.64 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 700/5435 batches | lr 0.0001 | ms/batch 350.33 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 800/5435 batches | lr 0.0001 | ms/batch 350.36 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 900/5435 batches | lr 0.0001 | ms/batch 350.55 | loss  0.07 | mse  0.07 |\n",
      "scGPT - INFO - | epoch   7 | 1000/5435 batches | lr 0.0001 | ms/batch 350.38 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1100/5435 batches | lr 0.0001 | ms/batch 350.68 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1200/5435 batches | lr 0.0001 | ms/batch 350.47 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1300/5435 batches | lr 0.0001 | ms/batch 350.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1400/5435 batches | lr 0.0001 | ms/batch 350.46 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1500/5435 batches | lr 0.0001 | ms/batch 350.79 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1600/5435 batches | lr 0.0001 | ms/batch 350.60 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1700/5435 batches | lr 0.0001 | ms/batch 350.42 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1800/5435 batches | lr 0.0001 | ms/batch 350.54 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 1900/5435 batches | lr 0.0001 | ms/batch 350.26 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2000/5435 batches | lr 0.0001 | ms/batch 350.71 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2100/5435 batches | lr 0.0001 | ms/batch 350.29 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2200/5435 batches | lr 0.0001 | ms/batch 350.34 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2300/5435 batches | lr 0.0001 | ms/batch 350.24 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2400/5435 batches | lr 0.0001 | ms/batch 350.51 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2500/5435 batches | lr 0.0001 | ms/batch 350.23 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2600/5435 batches | lr 0.0001 | ms/batch 350.56 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2700/5435 batches | lr 0.0001 | ms/batch 350.70 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2800/5435 batches | lr 0.0001 | ms/batch 350.37 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 2900/5435 batches | lr 0.0001 | ms/batch 350.79 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3000/5435 batches | lr 0.0001 | ms/batch 351.02 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3100/5435 batches | lr 0.0001 | ms/batch 350.81 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3200/5435 batches | lr 0.0001 | ms/batch 350.62 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3300/5435 batches | lr 0.0001 | ms/batch 350.92 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3400/5435 batches | lr 0.0001 | ms/batch 350.60 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3500/5435 batches | lr 0.0001 | ms/batch 350.63 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3600/5435 batches | lr 0.0001 | ms/batch 350.68 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3700/5435 batches | lr 0.0001 | ms/batch 350.62 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3800/5435 batches | lr 0.0001 | ms/batch 350.66 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 3900/5435 batches | lr 0.0001 | ms/batch 351.12 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4000/5435 batches | lr 0.0001 | ms/batch 351.03 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4100/5435 batches | lr 0.0001 | ms/batch 350.85 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4200/5435 batches | lr 0.0001 | ms/batch 350.89 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4300/5435 batches | lr 0.0001 | ms/batch 350.87 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4400/5435 batches | lr 0.0001 | ms/batch 351.07 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4500/5435 batches | lr 0.0001 | ms/batch 350.69 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4600/5435 batches | lr 0.0001 | ms/batch 350.60 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4700/5435 batches | lr 0.0001 | ms/batch 350.76 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4800/5435 batches | lr 0.0001 | ms/batch 350.50 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 4900/5435 batches | lr 0.0001 | ms/batch 350.55 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 5000/5435 batches | lr 0.0001 | ms/batch 350.95 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 5100/5435 batches | lr 0.0001 | ms/batch 351.04 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 5200/5435 batches | lr 0.0001 | ms/batch 350.87 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 5300/5435 batches | lr 0.0001 | ms/batch 350.51 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - | epoch   7 | 5400/5435 batches | lr 0.0001 | ms/batch 350.58 | loss  0.08 | mse  0.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 1937.14s | valid loss/mse 0.1339 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Early stop at epoch 7\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loader = pert_data.dataloader[\"train_loader\"]\n",
    "    valid_loader = pert_data.dataloader[\"val_loader\"]\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        train_loader,\n",
    "    )\n",
    "    val_loss, val_mre = evaluate(\n",
    "        model,\n",
    "        valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} |\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        save_dir / f\"model_{epoch}.pt\",\n",
    "    )\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")\n",
    "# save/dev_perturb_adamson-May12-13-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: TransformerGenerator, pert_list: List[str], pool_size: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Predict the gene expression values for the given perturbations.\n",
    "\n",
    "    Args:\n",
    "        model (:class:`torch.nn.Module`): The model to use for prediction.\n",
    "        pert_list (:obj:`List[str]`): The list of perturbations to predict.\n",
    "        pool_size (:obj:`int`, optional): For each perturbation, use this number\n",
    "            of cells in the control and predict their perturbation results. Report\n",
    "            the stats of these predictions. If `None`, use all control cells.\n",
    "    \"\"\"\n",
    "    adata = pert_data.adata\n",
    "    ctrl_adata = adata[adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    if pool_size is None:\n",
    "        pool_size = len(ctrl_adata.obs)\n",
    "    gene_list = pert_data.gene_names.values.tolist()\n",
    "    for pert in pert_list:\n",
    "        for i in pert:\n",
    "            if i not in gene_list:\n",
    "                raise ValueError(\n",
    "                    \"The gene is not in the perturbation graph. Please select from GEARS.gene_list!\"\n",
    "                )\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        results_pred = {}\n",
    "        for pert in pert_list:\n",
    "            cell_graphs = create_cell_graph_dataset_for_prediction(\n",
    "                pert, ctrl_adata, gene_list, device, num_samples=pool_size\n",
    "            )\n",
    "            loader = DataLoader(cell_graphs, batch_size=eval_batch_size, shuffle=False)\n",
    "            preds = []\n",
    "            for batch_data in loader:\n",
    "                pred_gene_values = model.pred_perturb(\n",
    "                    batch_data, include_zero_gene, gene_ids=gene_ids, amp=amp\n",
    "                )\n",
    "                preds.append(pred_gene_values)\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            results_pred[\"_\".join(pert)] = np.mean(preds.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    return results_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perturbation(\n",
    "    model: nn.Module, query: str, save_file: str = None, pool_size: int = None\n",
    "):\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sns.set_theme(style=\"ticks\", rc={\"axes.facecolor\": (0, 0, 0, 0)}, font_scale=1.5)\n",
    "\n",
    "    adata = pert_data.adata\n",
    "    gene2idx = pert_data.node_map\n",
    "    cond2name = dict(adata.obs[[\"condition\", \"condition_name\"]].values)\n",
    "    gene_raw2id = dict(zip(adata.var.index.values, adata.var.gene_name.values))\n",
    "\n",
    "    de_idx = [\n",
    "        gene2idx[gene_raw2id[i]]\n",
    "        for i in adata.uns[\"top_non_dropout_de_20\"][cond2name[query]]\n",
    "    ]\n",
    "    genes = [\n",
    "        gene_raw2id[i] for i in adata.uns[\"top_non_dropout_de_20\"][cond2name[query]]\n",
    "    ]\n",
    "    truth = adata[adata.obs.condition == query].X.toarray()[:, de_idx]\n",
    "    if query.split(\"+\")[1] == \"ctrl\":\n",
    "        pred = predict(model, [[query.split(\"+\")[0]]], pool_size=pool_size)\n",
    "        pred = pred[query.split(\"+\")[0]][de_idx]\n",
    "    else:\n",
    "        pred = predict(model, [query.split(\"+\")], pool_size=pool_size)\n",
    "        pred = pred[\"_\".join(query.split(\"+\"))][de_idx]\n",
    "    ctrl_means = adata[adata.obs[\"condition\"] == \"ctrl\"].to_df().mean()[de_idx].values\n",
    "\n",
    "    pred = pred - ctrl_means\n",
    "    truth = truth - ctrl_means\n",
    "\n",
    "    plt.figure(figsize=[16.5, 4.5])\n",
    "    plt.title(query)\n",
    "    plt.boxplot(truth, showfliers=False, medianprops=dict(linewidth=0))\n",
    "\n",
    "    for i in range(pred.shape[0]):\n",
    "        _ = plt.scatter(i + 1, pred[i], color=\"red\")\n",
    "\n",
    "    plt.axhline(0, linestyle=\"dashed\", color=\"green\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_ticklabels(genes, rotation=90)\n",
    "\n",
    "    plt.ylabel(\"Change in Gene Expression over Control\", labelpad=10)\n",
    "    plt.tick_params(axis=\"x\", which=\"major\", pad=5)\n",
    "    plt.tick_params(axis=\"y\", which=\"major\", pad=5)\n",
    "    sns.despine()\n",
    "\n",
    "    if save_file:\n",
    "        plt.savefig(save_file, bbox_inches=\"tight\", transparent=False)\n",
    "    # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1650x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict(best_model, [[\"FEV\"], [\"FEV\", \"SAMD11\"]])\n",
    "for p in perts_to_plot:\n",
    "    plot_perturbation(best_model, p, pool_size=300, save_file=f\"{save_dir}/{p}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perturb(\n",
    "    loader: DataLoader, model: TransformerGenerator, device: torch.device\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run model in inference mode using a given data loader\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    pert_cat = []\n",
    "    pred = []\n",
    "    truth = []\n",
    "    pred_de = []\n",
    "    truth_de = []\n",
    "    results = {}\n",
    "    logvar = []\n",
    "\n",
    "    for itr, batch in enumerate(loader):\n",
    "        batch.to(device)\n",
    "        pert_cat.extend(batch.pert)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)\n",
    "            t = batch.y\n",
    "            pred.extend(p.cpu())\n",
    "            truth.extend(t.cpu())\n",
    "\n",
    "            # Differentially expressed genes\n",
    "            for itr, de_idx in enumerate(batch.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "        # break\n",
    "\n",
    "    # all genes\n",
    "    results[\"pert_cat\"] = np.array(pert_cat)\n",
    "    pred = torch.stack(pred)\n",
    "    truth = torch.stack(truth)\n",
    "    results[\"pred\"] = pred.detach().cpu().numpy().astype(np.float64)\n",
    "    results[\"truth\"] = truth.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "    pred_de = torch.stack(pred_de)\n",
    "    truth_de = torch.stack(truth_de)\n",
    "    results[\"pred_de\"] = pred_de.detach().cpu().numpy().astype(np.float64)\n",
    "    results[\"truth_de\"] = truth_de.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.006556254728717275, 'mse_de': 0.13887629402417992, 'pearson': 0.9891767717041802, 'pearson_de': 0.9766728459367648}\n",
      "scGPT - INFO - test_combo_seen0_pearson_delta: nan\n",
      "scGPT - INFO - test_combo_seen0_pearson_delta_de: nan\n",
      "scGPT - INFO - test_combo_seen0_pearson_delta_top20_de_non_dropout: nan\n",
      "scGPT - INFO - test_combo_seen0_pearson_top20_de_non_dropout: nan\n",
      "scGPT - INFO - test_combo_seen1_pearson_delta: nan\n",
      "scGPT - INFO - test_combo_seen1_pearson_delta_de: nan\n",
      "scGPT - INFO - test_combo_seen1_pearson_delta_top20_de_non_dropout: nan\n",
      "scGPT - INFO - test_combo_seen1_pearson_top20_de_non_dropout: nan\n",
      "scGPT - INFO - test_combo_seen2_pearson_delta: nan\n",
      "scGPT - INFO - test_combo_seen2_pearson_delta_de: nan\n",
      "scGPT - INFO - test_combo_seen2_pearson_delta_top20_de_non_dropout: nan\n",
      "scGPT - INFO - test_combo_seen2_pearson_top20_de_non_dropout: nan\n",
      "scGPT - INFO - test_unseen_single_pearson_delta: 0.580450449840103\n",
      "scGPT - INFO - test_unseen_single_pearson_delta_de: 0.7584012143304197\n",
      "scGPT - INFO - test_unseen_single_pearson_delta_top20_de_non_dropout: 0.7578487105636055\n",
      "scGPT - INFO - test_unseen_single_pearson_top20_de_non_dropout: 0.9738074769690974\n"
     ]
    }
   ],
   "source": [
    "test_loader = pert_data.dataloader[\"test_loader\"]\n",
    "test_res = eval_perturb(test_loader, best_model, device)\n",
    "test_metrics, test_pert_res = compute_metrics(test_res)\n",
    "print(test_metrics)\n",
    "\n",
    "# save the dicts in json\n",
    "with open(f\"{save_dir}/test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f)\n",
    "with open(f\"{save_dir}/test_pert_res.json\", \"w\") as f:\n",
    "    json.dump(test_pert_res, f)\n",
    "\n",
    "deeper_res = deeper_analysis(pert_data.adata, test_res)\n",
    "non_dropout_res = non_dropout_analysis(pert_data.adata, test_res)\n",
    "\n",
    "metrics = [\"pearson_delta\", \"pearson_delta_de\"]\n",
    "metrics_non_dropout = [\n",
    "    \"pearson_delta_top20_de_non_dropout\",\n",
    "    \"pearson_top20_de_non_dropout\",\n",
    "]\n",
    "subgroup_analysis = {}\n",
    "for name in pert_data.subgroup[\"test_subgroup\"].keys():\n",
    "    subgroup_analysis[name] = {}\n",
    "    for m in metrics:\n",
    "        subgroup_analysis[name][m] = []\n",
    "\n",
    "    for m in metrics_non_dropout:\n",
    "        subgroup_analysis[name][m] = []\n",
    "\n",
    "for name, pert_list in pert_data.subgroup[\"test_subgroup\"].items():\n",
    "    for pert in pert_list:\n",
    "        for m in metrics:\n",
    "            subgroup_analysis[name][m].append(deeper_res[pert][m])\n",
    "\n",
    "        for m in metrics_non_dropout:\n",
    "            subgroup_analysis[name][m].append(non_dropout_res[pert][m])\n",
    "\n",
    "for name, result in subgroup_analysis.items():\n",
    "    for m in result.keys():\n",
    "        subgroup_analysis[name][m] = np.mean(subgroup_analysis[name][m])\n",
    "        logger.info(\"test_\" + name + \"_\" + m + \": \" + str(subgroup_analysis[name][m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt_2",
   "language": "python",
   "name": "scgpt_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
